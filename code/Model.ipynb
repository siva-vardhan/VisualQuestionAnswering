{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LAXFvRu4AjIA"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "import collections\n",
    "import tensorflow as tf \n",
    "import re\n",
    "import h5py\n",
    "import argparse\n",
    "import sys \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD PREPROCESSED TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary files: Here we have loaded file for training\n",
    "df_final = pickle.load(open('C:/Users/kasiv/Desktop/siva/project/train_df_final.pkl', 'rb'))\n",
    "features = pickle.load(open(''C:/Users/kasiv/Desktop/siva/project/image_feature_train.pkl', 'rb'))\n",
    "word_to_idx = pickle.load(open(''C:/Users/kasiv/Desktop/siva/project/word_to_idx.pkl', 'rb'))\n",
    "idx_to_word = pickle.load(open(''C:/Users/kasiv/Desktop/siva/project/idx_to_word.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WMrwspRWAjIo"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, merge, LSTM, Dropout, Dense, RepeatVector, BatchNormalization, \\\n",
    "    TimeDistributed, Flatten, Reshape\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1771,
     "status": "ok",
     "timestamp": 1526397490874,
     "user": {
      "displayName": "meghana kotagiri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108948321362643567286"
     },
     "user_tz": -330
    },
    "id": "Zpnvy_e5AjI2",
    "outputId": "c8d52205-ab4c-4fe5-99e6-3155ed3fd32b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\b160544me\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\b160544me\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model created\n",
      "Compiling model...\n",
      "Model compiled\n"
     ]
    }
   ],
   "source": [
    "PADDING_LEN=22\n",
    "vocabulary_size=len(word_to_idx)\n",
    "embed_size=150\n",
    "question_max_len=PADDING_LEN-1\n",
    "answer_max_len=PADDING_LEN-1\n",
    "# Image\n",
    "k_image_input = Input(shape=(4096,))\n",
    "k_image_repeat = RepeatVector(n=question_max_len)(k_image_input)\n",
    "\n",
    "# Question\n",
    "k_question_input = Input(shape=(question_max_len,), dtype='int32')\n",
    "k_question_embedded = Embedding(input_dim=vocabulary_size, output_dim=embed_size, input_length=question_max_len)(k_question_input)  \n",
    "k_question_embedded = Dropout(0.3)(k_question_embedded)\n",
    "\n",
    "# Merge\n",
    "\n",
    "from keras.layers import concatenate\n",
    "k_merged=concatenate([k_image_repeat, k_question_embedded], axis=-1)\n",
    "\n",
    "\n",
    "#from tensorflow.keras.layers import Concatenate\n",
    "#all_layers=[k_image_repeat, k_question_embedded]\n",
    "#k_merged=Concatenate()(all_layers)\n",
    "\n",
    "#k_merged = merge([k_image_repeat, k_question_embedded], mode='concat')  # Merge for layers merge for tensors\n",
    "k_merged = BatchNormalization()(k_merged)\n",
    "\n",
    "#Set encoder\n",
    "k_encoder_outputs, k_state_h, k_state_c  = LSTM(embed_size, return_state=True)(k_merged)\n",
    "k_encoder_outputs = Dropout(0.3)(k_encoder_outputs)\n",
    "k_encoder_states = [k_state_h, k_state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "k_decoder_inputs = Input(shape=(None, vocabulary_size))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "k_decoder_lstm = LSTM(embed_size, return_sequences=True, return_state=True)\n",
    "k_decoder_outputs, _, _ = k_decoder_lstm(k_decoder_inputs, initial_state=k_encoder_states)\n",
    "\n",
    "#Final Layer\n",
    "k_decoder_dense = Dense(vocabulary_size, activation='softmax')\n",
    "k_decoder_outputs = k_decoder_dense(k_decoder_outputs)\n",
    "\n",
    "model = Model([k_image_input, k_question_input, k_decoder_inputs], k_decoder_outputs)\n",
    "print('Model created')\n",
    "print('Compiling model...')\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "print('Model compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rIg9hcSTAjI9"
   },
   "outputs": [],
   "source": [
    "image_input=np.array(features)\n",
    "question_inputs=np.array(df_final['Q_Encoded'])\n",
    "question_inputs=np.array([np.array(item) for item in question_inputs])\n",
    "decoder_inputs =np.array(df_final['A_Encoded'])\n",
    "decoder_targets =np.array(df_final['A_Encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "stHJGreOAjJA"
   },
   "outputs": [],
   "source": [
    "m_decoder_inputs = np.zeros(\n",
    "    (len(decoder_inputs), PADDING_LEN-1, vocabulary_size),\n",
    "    dtype='float32')\n",
    "m_decoder_targets= np.zeros(\n",
    "    (len(decoder_targets), PADDING_LEN-1, vocabulary_size),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(question_inputs, decoder_inputs)):\n",
    "   # for t, char in enumerate(input_text):\n",
    "       # m_question_input_data[i, t, char] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        m_decoder_inputs[i, t, char] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            m_decoder_targets[i, t - 1, char] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3471
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 746465,
     "status": "ok",
     "timestamp": 1526398245787,
     "user": {
      "displayName": "meghana kotagiri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108948321362643567286"
     },
     "user_tz": -330
    },
    "id": "dsrPk7ghAjJI",
    "outputId": "3fffbffc-b3b0-4631-daed-27a1e9f6f002",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\b160544me\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\b160544me\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2553 samples, validate on 639 samples\n",
      "Epoch 1/10\n",
      "2553/2553 [==============================] - 22s 9ms/step - loss: 2.8888 - val_loss: 1.3671\n",
      "Epoch 2/10\n",
      "2553/2553 [==============================] - 21s 8ms/step - loss: 1.4058 - val_loss: 1.3534\n",
      "Epoch 3/10\n",
      "2553/2553 [==============================] - 20s 8ms/step - loss: 1.3302 - val_loss: 1.2747\n",
      "Epoch 4/10\n",
      "2553/2553 [==============================] - 20s 8ms/step - loss: 1.2742 - val_loss: 1.2709\n",
      "Epoch 5/10\n",
      "2553/2553 [==============================] - 20s 8ms/step - loss: 1.2251 - val_loss: 1.2570\n",
      "Epoch 6/10\n",
      "2553/2553 [==============================] - 20s 8ms/step - loss: 1.1833 - val_loss: 1.2487\n",
      "Epoch 7/10\n",
      "2553/2553 [==============================] - 20s 8ms/step - loss: 1.1397 - val_loss: 1.2339\n",
      "Epoch 8/10\n",
      "2553/2553 [==============================] - 20s 8ms/step - loss: 1.1010 - val_loss: 1.2326\n",
      "Epoch 9/10\n",
      "2553/2553 [==============================] - 20s 8ms/step - loss: 1.0570 - val_loss: 1.2469\n",
      "Epoch 10/10\n",
      "2553/2553 [==============================] - 20s 8ms/step - loss: 1.0243 - val_loss: 1.2145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x257f2b67708>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([features,question_inputs, m_decoder_inputs], m_decoder_targets, batch_size=100, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1526397094952,
     "user": {
      "displayName": "meghana kotagiri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108948321362643567286"
     },
     "user_tz": -330
    },
    "id": "626KfYHMj8FJ",
    "outputId": "6b5bac4a-3db0-45f4-c6bb-7fc41b5fb43a"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model([k_image_input,k_question_input], k_encoder_states)\n",
    "decoder_state_input_h = Input(shape=(embed_size,))\n",
    "decoder_state_input_c = Input(shape=(embed_size,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = k_decoder_lstm(k_decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = k_decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([k_decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dKbxNsgl2oV2"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(image_features,input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "  \n",
    "    states_value = encoder_model.predict([image_features,input_seq])\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, vocabulary_size))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 1] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx_to_word[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        decoded_sentence +=\" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '2' or\n",
    "           len(decoded_sentence) > PADDING_LEN-1):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, vocabulary_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD PREPROCESSED TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1526389448037,
     "user": {
      "displayName": "meghana kotagiri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108948321362643567286"
     },
     "user_tz": -330
    },
    "id": "ZiXFyaF-44yL",
    "outputId": "41c795f5-6f75-446f-c9b4-20614c1b7b9b"
   },
   "outputs": [],
   "source": [
    "#Open inputs of test dataset\n",
    "file = open('C:/Users/b160544me/Desktop/siva/project/image_feature_test.pkl', 'rb')\n",
    "features_test = pickle.load(file)\n",
    "file2 = open('C:/Users/b160544me/Desktop/siva/project/test_df_v_final.pkl', 'rb')\n",
    "df_test= pickle.load(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g1CgYY317OIv"
   },
   "outputs": [],
   "source": [
    "test_questions=np.array(df_test['Q_Encoded'])\n",
    "test_questions=np.array([np.array(item) for item in test_questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zd4IUqICJWZd"
   },
   "outputs": [],
   "source": [
    "#use model to predict the answer\n",
    "predicted_answers_test=[]\n",
    "for seq_index in range(len(df_test)):\n",
    "    input_seq = test_questions[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(features_test[seq_index: seq_index + 1], input_seq)\n",
    "    predicted_answers_test.append(decoded_sentence.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8466
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1214,
     "status": "ok",
     "timestamp": 1526397134106,
     "user": {
      "displayName": "meghana kotagiri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108948321362643567286"
     },
     "user_tz": -330
    },
    "id": "sxFhH1HjEP1I",
    "outputId": "cbb8982f-2b07-492b-cf6f-905153909eae"
   },
   "outputs": [],
   "source": [
    "predicted_answers_test=[item[0:len(item)-1] for item in predicted_answers_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gU5UiHfl8eeY"
   },
   "outputs": [],
   "source": [
    "#function used to remove end tokens from the predicted answers\n",
    "def find_end(item,st):\n",
    "    for i in range(0,len(item)):\n",
    "        if item[i]==st:\n",
    "            return i\n",
    "    return i+1  \n",
    "predicted_answers_test=[item[0:find_end(item,'<END>')] for item in predicted_answers_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 12852
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1526390575918,
     "user": {
      "displayName": "meghana kotagiri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108948321362643567286"
     },
     "user_tz": -330
    },
    "id": "0ZghtCFd9Pqv",
    "outputId": "a28a635a-707e-449f-edf6-e02f502c10c7"
   },
   "outputs": [],
   "source": [
    "#actual/target answers\n",
    "test_answers=np.array(df_test['A_parsed'])\n",
    "test_answers=[item for item in test_answers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for x,y in zip(test_answers,predicted_answers_test) if x == y) / len(test_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1606,
     "status": "ok",
     "timestamp": 1526397215547,
     "user": {
      "displayName": "meghana kotagiri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108948321362643567286"
     },
     "user_tz": -330
    },
    "id": "wUoB0zSK-D8w",
    "outputId": "689e45c8-d6e5-4afa-d6a6-95eacd932ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Final BLEU Score\n",
    "score = corpus_bleu(test_answers, predicted_answers_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(test_answers))\n",
    "print(np.shape(predicted_answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "keras ass.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
